So hopefully that was a good demonstration of what we're going to build, but it might be useful to talk about why we're building it.
So what you're looking at at the moment is an enterprise application that I've built using the BMAD method.
And one of the capabilities within it is to deal with AI prompt templates.
Now this is a full working system where we've got different sorts of prompts that we can look at.
As we go through we've got little template libraries of pre-designed prompt templates like predicates, yes or no, classifications or observations.
And all of these prompts are being used in areas like question generation for incidents.
So if something happened in a medical sort of environment with a customer with a...
So let's say something happened in a support environment with a...

So, imagine you're in a situation where you're a caregiver and what you're doing is giving
care to people with disabilities and we'll call that person Michael and the reporter,
the person giving the care is David in this particular situation.
And what happens during a session of work is that there's an event happened, some incident
of some sort, we got before, during, after, etc.
Now, if you filled in all this information yourself and especially if you're not really
good at doing writing, it might be useful if the AI could help you or support you by
generating a bunch of questions to elicit better information.

Now if you're the domain expert building the questions for this sort of system
you are going to want access to some sort of prompt testing framework so what
we've built behind the scenes with this application is the idea of having a
prompt to see what it would look like if it was interpolated with a bunch of
values and to see what values are available on the form that we're
actually editing right now.

So using this screen as basis, let's talk about what a prompt orchestration framework could have.
So we can have various prompts of course, and we might want to be able to copy them to the clipboard.
We want to see them as a template with interpolation values in the middle,
and those interpolation values would come from different data sources.
Here we've got concepts like incidents, and we should be able to have various fields,
and if they're green it means that we're actually using the field from the data source.
If they're blue in this case, we're not using them, but they are available.
We should be able to see how it looks if it was all interpolated,
and also be able to press copy on that and test it in any large language model that we like.
And we should be able to quickly iterate and test concepts.
So if we just paste in this idea of reporter name, caring for participant at location,
we should be able to see that information interpolated and run automatically in the testing harness.
And that's exactly what this application is doing.
So we've got some output format going on here, and if we were to press the complete step,
what would happen is it would generate a bunch of questions based on the prompt.

Now, as powerful as the AI prompt management system is, the problem with it is that it
was added into an application for which it's not a primary use case of the application.
This application is specifically about listing incidents, it's about creating new incidents.
And while having this prompt engineering framework built in and being able to dynamically change
on the fly has been useful, it really slows down the development of this system.
And so I thought when new capabilities were needed, why don't we go and create that using
a brand new system, and this is where the idea for Pome came about.